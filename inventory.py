import streamlit as st
import pandas as pd
from datetime import datetime, timedelta
import io
from supabase import create_client, Client
import math

# --- [1] ë¡œê·¸ì¸ ë³´ì•ˆ ---
def check_password():
    if 'password_correct' not in st.session_state:
        st.session_state.password_correct = False
    if st.session_state.password_correct:
        return True
    
    st.set_page_config(page_title="ì¬ê³ ê´€ë¦¬(ë¦¬ìŠ¤íŠ¸í˜•)", layout="wide")
    st.title("ğŸ­ ë””ì§€íƒ€ìŠ¤ ì°½ê³  ì¬ê³ ê´€ë¦¬ (Ver.12.4)")
    pwd = st.text_input("ë¹„ë°€ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”", type="password")
    if st.button("ë¡œê·¸ì¸"):
        if pwd == "1234": 
            st.session_state.password_correct = True
            st.rerun()
        else:
            st.error("ë¹„ë°€ë²ˆí˜¸ê°€ í‹€ë ¸ìŠµë‹ˆë‹¤.")
    return False

if not check_password():
    st.stop()

# --- [2] Supabase ì—°ê²° ì„¤ì • ---
@st.cache_resource
def init_connection():
    try:
        url = st.secrets["supabase"]["url"]
        key = st.secrets["supabase"]["key"]
        return create_client(url, key)
    except Exception as e:
        st.error(f"âŒ Supabase ì—°ê²° ì‹¤íŒ¨: {e}")
        return None

supabase = init_connection()

# --- [ê³µí†µ] ëŒ€ìš©ëŸ‰ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ---
def fetch_all_data(table_name, sort_col):
    if not supabase: return []
    all_data = []
    page_size = 1000
    offset = 0
    
    while True:
        try:
            response = supabase.table(table_name).select("*").order(sort_col).range(offset, offset + page_size - 1).execute()
            data = response.data
            if not data: break
            all_data.extend(data)
            if len(data) < page_size: break
            offset += page_size
        except Exception as e:
            print(f"Error fetching {table_name}: {e}")
            break
    return all_data

# --- [3-A] ë¬´ê±°ìš´ ë°ì´í„° ë¡œë“œ ---
@st.cache_data(ttl=21600, show_spinner=False)
def load_heavy_data():
    if not supabase: return pd.DataFrame(), pd.DataFrame(), pd.DataFrame()
    try:
        data_m = fetch_all_data("í’ˆëª©í‘œ", "í’ˆëª©ì½”ë“œ")
        df_m = pd.DataFrame(data_m)
        
        data_map = fetch_all_data("ë§¤í•‘ì •ë³´", "boxë²ˆí˜¸")
        df_map = pd.DataFrame(data_map)
        
        data_d = fetch_all_data("ìƒì„¸ë‚´ì—­", "boxë²ˆí˜¸")
        df_d = pd.DataFrame(data_d) 

        for df in [df_m, df_map, df_d]:
            if not df.empty:
                df.columns = [c.lower() for c in df.columns]
        return df_m, df_map, df_d
    except Exception:
        return pd.DataFrame(), pd.DataFrame(), pd.DataFrame()

# --- [3-B] ê°€ë²¼ìš´ ë°ì´í„° ë¡œë“œ ---
@st.cache_data(ttl=600, show_spinner=False)
def load_light_data():
    if not supabase: return pd.DataFrame()
    try:
        data_l = fetch_all_data("ì…ì¶œê³ ", "id")
        df_l = pd.DataFrame(data_l)
        if not df_l.empty:
            df_l.columns = [c.lower() for c in df_l.columns]
        return df_l
    except Exception:
        return pd.DataFrame()

def clear_cache_all():
    st.cache_data.clear()

# --- [4] ì¬ê³  í˜„í™© ê³„ì‚° ---
@st.cache_data(show_spinner=False)
def calculate_stock_snapshot(df_log, df_mapping, df_master, df_details):
    if df_log.empty: return pd.DataFrame(), pd.DataFrame(), pd.DataFrame()

    last_stat = df_log.sort_values('id').groupby('boxë²ˆí˜¸').tail(1)
    stock_boxes = last_stat[last_stat['êµ¬ë¶„'].isin(['ì…ê³ ', 'ì´ë™'])].copy()
    
    if not stock_boxes.empty:
        stock_boxes['match_key'] = stock_boxes['boxë²ˆí˜¸'].astype(str).str.strip().str.upper()
    else:
        return pd.DataFrame(), pd.DataFrame(), pd.DataFrame() 
    
    if df_mapping.empty:
        df_mapping = pd.DataFrame(columns=['match_key', 'boxë²ˆí˜¸', 'í’ˆëª©ì½”ë“œ', 'ìˆ˜ëŸ‰'])
    else:
        if 'boxë²ˆí˜¸' in df_mapping.columns:
            df_mapping['match_key'] = df_mapping['boxë²ˆí˜¸'].astype(str).str.strip().str.upper()
            if 'í’ˆëª©ì½”ë“œ' in df_mapping.columns:
                df_mapping['í’ˆëª©ì½”ë“œ'] = df_mapping['í’ˆëª©ì½”ë“œ'].astype(str).str.strip().str.upper()
        else:
            df_mapping['match_key'] = ""

    if not df_master.empty and 'í’ˆëª©ì½”ë“œ' in df_master.columns:
        df_master['í’ˆëª©ì½”ë“œ'] = df_master['í’ˆëª©ì½”ë“œ'].astype(str).str.strip().str.upper()

    merged = pd.merge(stock_boxes, df_mapping, on='match_key', how='left', suffixes=('', '_map'))
    merged['ìœ„ì¹˜'] = merged['ìœ„ì¹˜'].fillna('ë¯¸ì§€ì •').replace('', 'ë¯¸ì§€ì •')
    merged['íŒŒë ›íŠ¸'] = merged['íŒŒë ›íŠ¸'].fillna('ì´ë¦„ì—†ìŒ').replace('', 'ì´ë¦„ì—†ìŒ')
    
    if not df_master.empty and 'í’ˆëª©ì½”ë“œ' in merged.columns:
        merged = pd.merge(merged, df_master, on='í’ˆëª©ì½”ë“œ', how='left')

    filtered_details = pd.DataFrame()
    if not df_details.empty and 'boxë²ˆí˜¸' in df_details.columns:
        df_details['match_key'] = df_details['boxë²ˆí˜¸'].astype(str).str.strip().str.upper()
        active_keys = stock_boxes['match_key'].unique()
        filtered_details = df_details[df_details['match_key'].isin(active_keys)].copy()
        
        loc_info = stock_boxes[['match_key', 'ìœ„ì¹˜', 'íŒŒë ›íŠ¸']]
        filtered_details = pd.merge(filtered_details, loc_info, on='match_key', how='left')
        
        if 'match_key' in filtered_details.columns: del filtered_details['match_key']
            
    return stock_boxes, merged, filtered_details

# --- ë°ì´í„° ì—…ë¡œë“œ ---
def chunked_upsert(table_name, df, key_col, batch_size=5000):
    if not supabase: return False
    if df.empty: return False
    try:
        df = df.astype(str)
        if key_col in df.columns: df[key_col] = df[key_col].str.strip().str.upper()
        df = df.where(pd.notnull(df), None)
        total_rows = len(df)
        chunks = math.ceil(total_rows / batch_size)
        my_bar = st.progress(0, text=f"{table_name} ì—…ë¡œë“œ...")
        for i in range(chunks):
            start = i * batch_size
            end = start + batch_size
            chunk = df.iloc[start:end]
            data = chunk.to_dict(orient='records')
            supabase.table(table_name).upsert(data, on_conflict=key_col).execute()
            my_bar.progress(min((i+1)/chunks, 1.0))
        my_bar.empty()
        return True
    except Exception as e:
        st.error(f"ì‹¤íŒ¨: {e}")
        return False

def chunked_insert(table_name, df, batch_size=5000):
    if not supabase: return False
    if df.empty: return False
    try:
        df = df.where(pd.notnull(df), None)
        total_rows = len(df)
        chunks = math.ceil(total_rows / batch_size)
        my_bar = st.progress(0, text=f"{table_name} ì¶”ê°€...")
        for i in range(chunks):
            start = i * batch_size
            end = start + batch_size
            chunk = df.iloc[start:end]
            data = chunk.to_dict(orient='records')
            supabase.table(table_name).insert(data).execute()
            my_bar.progress(min((i+1)/chunks, 1.0))
        my_bar.empty()
        return True
    except Exception as e:
        st.error(f"ì‹¤íŒ¨: {e}")
        return False

def insert_log(new_data_list):
    if not supabase: return False
    try:
        cleaned_list = []
        for item in new_data_list:
            cleaned_list.append({
                "ë‚ ì§œ": item.get("ë‚ ì§œ"),
                "êµ¬ë¶„": item.get("êµ¬ë¶„"),
                "ì…ê³ êµ¬ë¶„": item.get("ì…ê³ êµ¬ë¶„", ""),
                "boxë²ˆí˜¸": str(item.get("Boxë²ˆí˜¸")).strip().upper(), 
                "ìœ„ì¹˜": item.get("ìœ„ì¹˜", ""),
                "íŒŒë ›íŠ¸": item.get("íŒŒë ›íŠ¸", "")
            })
        supabase.table("ì…ì¶œê³ ").insert(cleaned_list).execute()
        st.cache_data.clear()
        return True
    except Exception as e:
        st.error(f"ì‹¤íŒ¨: {e}")
        return False

# --- ì¼ì • ê´€ë¦¬ (Native) ---
def fetch_schedules_native():
    if not supabase: return []
    try:
        res = supabase.table("schedule").select("*").order("start_time", desc=True).execute()
        return res.data
    except Exception as e:
        return []

def add_schedule(title, start_time):
    if not supabase: return
    try:
        supabase.table("schedule").insert({"title": title, "start_time": start_time}).execute()
        return True
    except Exception as e:
        st.error(f"ì¶”ê°€ ì‹¤íŒ¨: {e}")
        return False

def delete_schedule(id):
    if not supabase: return
    try:
        supabase.table("schedule").delete().eq("id", id).execute()
        return True
    except Exception as e:
        st.error(f"ì‚­ì œ ì‹¤íŒ¨: {e}")
        return False

# --- ìœ í‹¸ë¦¬í‹° ---
def init_session_state():
    if 'scan_buffer' not in st.session_state: st.session_state.scan_buffer = []
    if 'proc_msg' not in st.session_state: st.session_state.proc_msg = None
    if 'selected_rack' not in st.session_state: st.session_state.selected_rack = None

def to_excel(df):
    output = io.BytesIO()
    with pd.ExcelWriter(output, engine='openpyxl') as writer:
        df.to_excel(writer, index=False, sheet_name='Sheet1')
    return output.getvalue()

def get_sample_file():
    sample_data = {'ë‚ ì§œ': [datetime.now().strftime("%Y-%m-%d %H:%M:%S")],'ì´ë™êµ¬ë¶„': ['ì…ê³ '],'ì…ê³ êµ¬ë¶„': ['ì¼ë°˜ì² ê±°'],'Boxë²ˆí˜¸': ['V2024...'],'ìœ„ì¹˜': ['1-2-7'],'íŒŒë ›íŠ¸': ['P-01']}
    return to_excel(pd.DataFrame(sample_data))

def buffer_scan(df_master, df_mapping, df_log, df_details):
    scan_val = str(st.session_state.scan_input).strip().upper()
    mode = st.session_state.work_mode
    curr_loc = str(st.session_state.get('curr_location', '')).strip()
    curr_pal = str(st.session_state.get('curr_palette', '')).strip()
    if not scan_val: return

    disp_name, disp_spec, disp_qty, p_code = "ì •ë³´ì—†ìŒ", "ê·œê²©ì—†ìŒ", 0, ""
    if not df_mapping.empty and 'boxë²ˆí˜¸' in df_mapping.columns:
        df_mapping['temp_key'] = df_mapping['boxë²ˆí˜¸'].astype(str).str.strip().str.upper()
        map_info = df_mapping[df_mapping['temp_key'] == scan_val]
        if not map_info.empty:
            p_code = str(map_info.iloc[0]['í’ˆëª©ì½”ë“œ']).strip()
            disp_qty = map_info.iloc[0]['ìˆ˜ëŸ‰']
            if not df_master.empty and 'í’ˆëª©ì½”ë“œ' in df_master.columns:
                df_master['temp_key'] = df_master['í’ˆëª©ì½”ë“œ'].astype(str).str.strip().str.upper()
                m_info = df_master[df_master['temp_key'] == p_code.upper()]
                if not m_info.empty:
                    disp_name = m_info.iloc[0]['í’ˆëª…']
                    disp_spec = m_info.iloc[0]['ê·œê²©']

    is_compressed = False
    target_box_no = scan_val
    if p_code == "ì •ë³´ì—†ìŒ":
        if not df_details.empty and 'ì••ì¶•ì½”ë“œ' in df_details.columns:
            df_details['temp_code'] = df_details['ì••ì¶•ì½”ë“œ'].astype(str).str.strip().str.upper()
            matched = df_details[df_details['temp_code'] == scan_val]
            if not matched.empty:
                target_box_no = str(matched.iloc[0]['boxë²ˆí˜¸']).strip().upper()
                is_compressed = True
                if not df_mapping.empty:
                    df_mapping['temp_key'] = df_mapping['boxë²ˆí˜¸'].astype(str).str.strip().str.upper()
                    map_info = df_mapping[df_mapping['temp_key'] == target_box_no]
                    if not map_info.empty:
                        p_code = str(map_info.iloc[0]['í’ˆëª©ì½”ë“œ']).strip()
                        disp_qty = map_info.iloc[0]['ìˆ˜ëŸ‰']
                        if not df_master.empty:
                            m_info = df_master[df_master['temp_key'] == p_code.upper()]
                            if not m_info.empty:
                                disp_name = m_info.iloc[0]['í’ˆëª…']
                                disp_spec = m_info.iloc[0]['ê·œê²©']

    box_status, current_db_loc = "ì‹ ê·œ", "ë¯¸ì§€ì •"
    if not df_log.empty and 'boxë²ˆí˜¸' in df_log.columns:
        df_log['temp_key'] = df_log['boxë²ˆí˜¸'].astype(str).str.strip().str.upper()
        my_logs = df_log[df_log['temp_key'] == target_box_no]
        if not my_logs.empty:
            last_log = my_logs.iloc[0]
            last_action = last_log['êµ¬ë¶„']
            current_db_loc = last_log['ìœ„ì¹˜']
            if last_action in ['ì…ê³ ', 'ì´ë™']: box_status = f"ì°½ê³ ìˆìŒ({current_db_loc})"
            elif last_action == 'ì¶œê³ ': box_status = "ì¶œê³ ë¨"

    is_duplicate = (mode == "ì…ê³ " and "ì°½ê³ ìˆìŒ" in box_status)
    now_str = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    msg_prefix = "ğŸ“¦ ì••ì¶•ì½”ë“œ ì¸ì‹ â†’ " if is_compressed else ""
    
    if mode == "ì¡°íšŒ(ê²€ìƒ‰)":
        msg_text = f"ğŸ” {msg_prefix}Box: {target_box_no} / {disp_name} / {disp_spec} / {disp_qty}ê°œ / {current_db_loc}"
        st.session_state.proc_msg = ("info", msg_text)
    elif mode == "ì¶œê³ ":
        if "ì°½ê³ ìˆìŒ" not in box_status:
            st.session_state.proc_msg = ("error", f"â›” ì¶œê³  ë¶ˆê°€: Box [{target_box_no}] ì¬ê³  ì—†ìŒ")
        else:
            log_entry = {'ë‚ ì§œ': now_str, 'êµ¬ë¶„': mode, 'ì…ê³ êµ¬ë¶„': '', 'Boxë²ˆí˜¸': target_box_no, 'í’ˆëª©ì½”ë“œ': p_code, 'ê·œê²©': disp_spec, 'ìˆ˜ëŸ‰': disp_qty, 'ìœ„ì¹˜': final_loc, 'íŒŒë ›íŠ¸': final_pal}
            st.session_state.scan_buffer.append(log_entry)
            st.session_state.proc_msg = ("success", f"âœ… {msg_prefix}ì¶œê³  ëŒ€ê¸°: {target_box_no}")
    else: 
        if is_duplicate:
            st.session_state.proc_msg = ("error", f"â›” ì´ë¯¸ ì…ê³ ë¨: {target_box_no}")
        else:
            final_loc = curr_loc if curr_loc else "ë¯¸ì§€ì •"
            final_pal = curr_pal if curr_pal else "ì´ë¦„ì—†ìŒ"
            log_entry = {'ë‚ ì§œ': now_str, 'êµ¬ë¶„': mode, 'ì…ê³ êµ¬ë¶„': '', 'Boxë²ˆí˜¸': target_box_no, 'í’ˆëª©ì½”ë“œ': p_code, 'ê·œê²©': disp_spec, 'ìˆ˜ëŸ‰': disp_qty, 'ìœ„ì¹˜': final_loc, 'íŒŒë ›íŠ¸': final_pal}
            st.session_state.scan_buffer.append(log_entry)
            st.session_state.proc_msg = ("success", f"âœ… {msg_prefix}{mode}: {target_box_no}")
            
    st.session_state.scan_input = ""

# --- ë©”ì¸ ---
def main():
    init_session_state()
    
    with st.spinner("ğŸ“¦ ê¸°ì´ˆ ë°ì´í„° ë¡œë“œ ì¤‘..."):
        df_master, df_mapping, df_details = load_heavy_data()
        
    df_log = load_light_data()

    tab1, tab2, tab3, tab4, tab5, tab6, tab7 = st.tabs(["1. ì—°ì† ìŠ¤ìº”", "2. ì¬ê³  í˜„í™©", "3. ì¼ê´„ ì—…ë¡œë“œ", "4. í¬ì¥ë°ì´í„°", "5. í’ˆëª© ë§ˆìŠ¤í„°", "6. ë°ì´í„° ì§„ë‹¨", "7. ì›”ê°„ ì¼ì •"])

    with tab1:
        c_h, c_r = st.columns([4, 1])
        with c_h: st.subheader("ğŸš€ ìŠ¤ìº” ì‘ì—…")
        with c_r: 
            if st.button("ğŸ”„ ìƒˆë¡œê³ ì¹¨", use_container_width=True, key='r1'): clear_cache_all(); st.rerun()

        if st.session_state.proc_msg:
            m_type, m_text = st.session_state.proc_msg
            if m_type == 'success': st.success(m_text)
            elif m_type == 'error': st.error(m_text)
            else: st.info(m_text)

        c1, c2, c3, c4 = st.columns([1.5, 1, 1, 2])
        with c1: st.radio("ëª¨ë“œ", ["ì…ê³ ", "ì¬ê³ ì´ë™", "ì¶œê³ ", "ì¡°íšŒ(ê²€ìƒ‰)"], horizontal=True, key="work_mode")
        with c2: st.text_input("ì ì¬ ìœ„ì¹˜ (1-2-7)", key="curr_location")
        with c3: st.text_input("íŒŒë ›íŠ¸ ì´ë¦„", key="curr_palette")
        with c4: st.text_input("Box ë²ˆí˜¸ ë˜ëŠ” ì••ì¶•ì½”ë“œ ìŠ¤ìº”", key="scan_input", on_change=buffer_scan, args=(df_master, df_mapping, df_log, df_details))

        if st.session_state.scan_buffer:
            disp_df = pd.DataFrame(st.session_state.scan_buffer)
            cols_order = ['ë‚ ì§œ', 'êµ¬ë¶„', 'ì…ê³ êµ¬ë¶„', 'Boxë²ˆí˜¸', 'í’ˆëª©ì½”ë“œ', 'ê·œê²©', 'ìˆ˜ëŸ‰', 'ìœ„ì¹˜', 'íŒŒë ›íŠ¸']
            final_cols = [c for c in cols_order if c in disp_df.columns]
            st.dataframe(disp_df[final_cols].iloc[::-1], use_container_width=True)
            
            csv_data = to_excel(disp_df[final_cols])
            st.download_button("ğŸ“¥ ìŠ¤ìº” ëª©ë¡ ë‹¤ìš´ë¡œë“œ", data=csv_data, file_name=f"ìŠ¤ìº”ëª©ë¡_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx", mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
        else: st.info("ëŒ€ê¸° ì¤‘...")
        
        if st.button("ğŸ’¾ DBì— ì €ì¥ (ë¹ ë¦„)", type="primary", use_container_width=True): 
            if insert_log(st.session_state.scan_buffer):
                st.session_state.scan_buffer = []
                st.session_state.proc_msg = ("success", "âœ… ì €ì¥ ì™„ë£Œ!")
                st.rerun()
        if st.button("ğŸ—‘ï¸ ëŒ€ê¸° ëª©ë¡ ë¹„ìš°ê¸°", use_container_width=True): st.session_state.scan_buffer = []

        st.divider()
        st.subheader("ğŸ“Š ìµœê·¼ ì…ì¶œê³  ì´ë ¥ (ì „ì²´)")
        if not df_log.empty:
            csv_data = to_excel(df_log)
            st.download_button("ğŸ“¥ ì „ì²´ ì…ì¶œê³  ì´ë ¥ ë‹¤ìš´ë¡œë“œ", data=csv_data, file_name=f"ì „ì²´ì´ë ¥_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx", mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
            st.dataframe(df_log.head(1000), use_container_width=True)
        else: st.info("ì´ë ¥ì´ ì—†ìŠµë‹ˆë‹¤.")

    with tab2:
        view_inventory_dashboard(df_log, df_mapping, df_master, df_details)

    with tab3:
        st.subheader("ğŸ“¤ ì…ì¶œê³  ë‚´ì—­ ì¼ê´„ ì—…ë¡œë“œ")
        st.download_button("ğŸ“¥ ìƒ˜í”Œ ì–‘ì‹ ë‹¤ìš´ë¡œë“œ", get_sample_file(), "ì…ì¶œê³ _ìƒ˜í”Œ.xlsx")
        st.info("ì–‘ì‹: ë‚ ì§œ / ì´ë™êµ¬ë¶„ / ì…ê³ êµ¬ë¶„ / Boxë²ˆí˜¸ / ìœ„ì¹˜ / íŒŒë ›íŠ¸")
        
        up = st.file_uploader("ì—‘ì…€ íŒŒì¼", type=['xlsx', 'csv'])
        if up and st.button("DB ì—…ë¡œë“œ (ëŒ€ìš©ëŸ‰ ëŒ€ì‘)"):
            try:
                df = pd.read_excel(up) if up.name.endswith('xlsx') else pd.read_csv(up)
                clean_df = pd.DataFrame()
                
                df.columns = df.columns.str.strip().str.replace(' ', '')
                col_box = next((c for c in df.columns if 'box' in c.lower() or 'ë°•ìŠ¤' in c), None)
                if not col_box:
                    st.error("âŒ 'Boxë²ˆí˜¸' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                    st.stop()
                
                col_gubun = next((c for c in df.columns if ('ì´ë™êµ¬ë¶„' in c) or ('êµ¬ë¶„' in c and 'ì…ê³ ' not in c)), None)
                col_in_type = next((c for c in df.columns if 'ì…ê³ êµ¬ë¶„' in c), None)
                col_loc = next((c for c in df.columns if 'ìœ„ì¹˜' in c), None)
                col_pal = next((c for c in df.columns if 'íŒŒë ›íŠ¸' in c or 'íŒ”ë ˆíŠ¸' in c), None)
                col_date = next((c for c in df.columns if 'ë‚ ì§œ' in c), None)

                if col_date: clean_df['ë‚ ì§œ'] = df[col_date].astype(str).replace('nan', datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
                else: clean_df['ë‚ ì§œ'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                
                clean_df['êµ¬ë¶„'] = df[col_gubun].astype(str) if col_gubun else 'ì…ê³ '
                clean_df['ì…ê³ êµ¬ë¶„'] = df[col_in_type].astype(str).replace('nan', '') if col_in_type else ''
                clean_df['boxë²ˆí˜¸'] = df[col_box].astype(str).str.strip().str.upper()
                clean_df['ìœ„ì¹˜'] = df[col_loc].astype(str).replace('nan', '') if col_loc else ''
                clean_df['íŒŒë ›íŠ¸'] = df[col_pal].astype(str).replace('nan', '') if col_pal else ''

                current_stock, _, _ = calculate_stock_snapshot(df_log, df_mapping, df_master, df_details)
                available_boxes = set(current_stock['match_key'].values) if not current_stock.empty else set()
                outbound_check = clean_df[clean_df['êµ¬ë¶„'] == 'ì¶œê³ ']
                missing_boxes = [b for b in outbound_check['boxë²ˆí˜¸'] if b not in available_boxes]
                
                if missing_boxes:
                    st.error(f"â›” ì—…ë¡œë“œ ë¶ˆê°€: ë‹¤ìŒ ë°•ìŠ¤ë“¤ì€ í˜„ì¬ ì¬ê³ ì— ì—†ì–´ ì¶œê³ í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n{missing_boxes[:10]} ...")
                    st.stop()

                if chunked_insert('ì…ì¶œê³ ', clean_df):
                    st.success(f"âœ… ì´ {len(clean_df)}ê±´ ì—…ë¡œë“œ ì™„ë£Œ!")
                    clear_cache_all()
                    st.rerun()
            except Exception as e:
                st.error(f"ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    with tab4:
        st.subheader("ğŸ“¦ í¬ì¥ë°ì´í„°(ë§ˆìŠ¤í„°) ë“±ë¡ (ëŒ€ìš©ëŸ‰)")
        with st.expander("ğŸš¨ ë°ì´í„° ì „ì²´ ì´ˆê¸°í™” (ì£¼ì˜)"):
            st.warning("ì´ ë²„íŠ¼ì„ ëˆ„ë¥´ë©´ ëª¨ë“  ë°ì´í„°ê°€ ì‚­ì œë©ë‹ˆë‹¤.")
            if st.button("ë°ì´í„° ì´ˆê¸°í™” ì‹¤í–‰", type="primary"):
                if reset_database():
                    st.success("ëª¨ë“  ë°ì´í„°ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")
                    st.rerun()

        up_pack = st.file_uploader("í¬ì¥ íŒŒì¼ (.xlsx)", type=['xlsx'])
        if up_pack and st.button("ë“±ë¡ (ëŒ€ìš©ëŸ‰)"):
            try:
                raw = pd.read_excel(up_pack, dtype=str)
                raw = raw.applymap(lambda x: x.strip() if isinstance(x, str) else x)
                
                grp = raw.groupby(['ì¹´í†¤ë°•ìŠ¤ë²ˆí˜¸', 'ë°•ìŠ¤ìì¬ì½”ë“œ']).size().reset_index(name='ìˆ˜ëŸ‰')
                grp.columns = ['boxë²ˆí˜¸', 'í’ˆëª©ì½”ë“œ', 'ìˆ˜ëŸ‰']
                grp['boxë²ˆí˜¸'] = grp['boxë²ˆí˜¸'].str.upper()
                
                dets = pd.DataFrame(columns=['boxë²ˆí˜¸', 'í’ˆëª©ì½”ë“œ', 'ê·œê²©', 'ì••ì¶•ì½”ë“œ'])
                if 'ì••ì¶•ì½”ë“œ' in raw.columns:
                    dets = raw[['ì¹´í†¤ë°•ìŠ¤ë²ˆí˜¸', 'ë°•ìŠ¤ìì¬ì½”ë“œ', 'ë°•ìŠ¤ìì¬ê·œê²©', 'ì••ì¶•ì½”ë“œ']].copy()
                    dets.columns = ['boxë²ˆí˜¸', 'í’ˆëª©ì½”ë“œ', 'ê·œê²©', 'ì••ì¶•ì½”ë“œ']
                    dets['boxë²ˆí˜¸'] = dets['boxë²ˆí˜¸'].str.upper()

                items = raw[['ë°•ìŠ¤ìì¬ì½”ë“œ', 'ë°•ìŠ¤ìì¬ëª…', 'ë°•ìŠ¤ìì¬ê·œê²©', 'ì¶œê³ ì²˜ëª…']].drop_duplicates('ë°•ìŠ¤ìì¬ì½”ë“œ')
                items.columns = ['í’ˆëª©ì½”ë“œ', 'í’ˆëª…', 'ê·œê²©', 'ê³µê¸‰ì—…ì²´']
                items['í’ˆëª©ì½”ë“œ'] = items['í’ˆëª©ì½”ë“œ'].str.upper()
                items['ë¶„ë¥˜êµ¬ë¶„'] = ''
                items['ë°”ì½”ë“œ'] = ''

                st.write("í’ˆëª©í‘œ ì—…ë¡œë“œ ì¤‘...")
                chunked_upsert('í’ˆëª©í‘œ', items, 'í’ˆëª©ì½”ë“œ')
                
                st.write("ë§¤í•‘ì •ë³´ ì—…ë¡œë“œ ì¤‘...")
                chunked_upsert('ë§¤í•‘ì •ë³´', grp, 'boxë²ˆí˜¸')
                
                if not dets.empty:
                    st.write("ìƒì„¸ë‚´ì—­ ì—…ë¡œë“œ ì¤‘...")
                    chunked_insert('ìƒì„¸ë‚´ì—­', dets)
                
                clear_cache_all()
                st.success("âœ… ëŒ€ìš©ëŸ‰ ë“±ë¡ ì™„ë£Œ!")
                st.rerun()
            except Exception as e: st.error(f"ì˜¤ë¥˜: {e}")

    with tab5:
        st.dataframe(df_master.head(1000))

    with tab6:
        st.subheader("ğŸ•µï¸â€â™€ï¸ ë°ì´í„° ì§„ë‹¨ (ì´ëŸ‰ í™•ì¸)")
        
        if st.button("ğŸ”„ [í•„ìˆ˜] ìºì‹œ ì‚­ì œ ë° ë°ì´í„° ì¬ë¡œë“œ", type="primary", use_container_width=True):
            clear_cache_all()
            st.rerun()
            
        c1, c2, c3 = st.columns(3)
        c1.metric("í’ˆëª©í‘œ", f"{len(df_master)}ê±´")
        c2.metric("ë§¤í•‘ì •ë³´", f"{len(df_mapping)}ê±´")
        c3.metric("ì…ì¶œê³ ", f"{len(df_log)}ê±´")
        st.write("â–¼ ë§¤í•‘ì •ë³´ ìƒ˜í”Œ")
        st.dataframe(df_mapping.head(50))

    with tab7:
        st.subheader("ğŸ—“ï¸ ì›”ê°„ ì¶œê³  ì¼ì • (ë¦¬ìŠ¤íŠ¸í˜•)")
        c1, c2 = st.columns([1, 2])
        
        with c1:
            st.markdown("##### âœï¸ ì¼ì • ë“±ë¡")
            sel_date = st.date_input("ë‚ ì§œ ì„ íƒ", value=datetime.now())
            evt_title = st.text_input("ì—…ì²´ëª… / ë‚´ìš©")
            evt_time = st.time_input("ì‹œê°„", value=datetime.now().time())
            
            if st.button("ì¼ì • ì¶”ê°€", type="primary", use_container_width=True):
                if evt_title:
                    final_dt = datetime.combine(sel_date, evt_time).isoformat()
                    if add_schedule(evt_title, final_dt):
                        st.success("âœ… ë“±ë¡ë¨")
                        st.rerun()
                else:
                    st.warning("ë‚´ìš©ì„ ì…ë ¥í•˜ì„¸ìš”.")

        with c2:
            st.markdown(f"##### ğŸ“‹ {sel_date.strftime('%Y-%m-%d')} ì¼ì • ëª©ë¡")
            all_schedules = fetch_schedules_native()
            daily_events = []
            
            # [ìˆ˜ì •] ë‚ ì§œ ë³€í™˜ ì˜¤ë¥˜ ë°©ì§€ ë¡œì§ ì ìš©
            for s in all_schedules:
                try:
                    # ìœ ì—°í•œ ë‚ ì§œ íŒŒì‹± (Pandas í™œìš©)
                    dt = pd.to_datetime(s['start_time']).to_pydatetime()
                    if dt.date() == sel_date:
                        s['parsed_time'] = dt # íŒŒì‹±ëœ ì‹œê°„ ì €ì¥
                        daily_events.append(s)
                except Exception:
                    continue # ë‚ ì§œ í˜•ì‹ ê¹¨ì§„ê±´ ë¬´ì‹œ
            
            if daily_events:
                for evt in daily_events:
                    with st.expander(f"{evt['title']} ({evt['parsed_time'].strftime('%H:%M')})"):
                        if st.button("ì‚­ì œ", key=f"del_{evt['id']}", type="secondary"):
                            if delete_schedule(evt['id']):
                                st.rerun()
            else:
                st.info("í•´ë‹¹ ë‚ ì§œì— ì¼ì •ì´ ì—†ìŠµë‹ˆë‹¤.")

        st.divider()
        st.markdown("##### ğŸ“… ì „ì²´ ì¼ì • ë¦¬ìŠ¤íŠ¸ (ìµœì‹ ìˆœ)")
        if all_schedules:
            df_sched = pd.DataFrame(all_schedules)
            # [í•µì‹¬ ìˆ˜ì •] ì—ëŸ¬ê°€ ë‚¬ë˜ ë¶€ë¶„ì„ Pandasì˜ ê°•ë ¥í•œ to_datetimeìœ¼ë¡œ êµì²´
            # errors='coerce'ëŠ” ë³€í™˜ ì•ˆ ë˜ëŠ” ì´ìƒí•œ ê°’ì€ NaT(ë¹ˆê°’)ë¡œ ë§Œë“¤ì–´ë²„ë¦¼ -> ì—ëŸ¬ ì•ˆ ë‚¨
            df_sched['dt_obj'] = pd.to_datetime(df_sched['start_time'], errors='coerce')
            df_sched['ë‚ ì§œ'] = df_sched['dt_obj'].dt.strftime('%Y-%m-%d %H:%M').fillna("ë‚ ì§œ ì˜¤ë¥˜")
            
            st.dataframe(df_sched[['ë‚ ì§œ', 'title']], use_container_width=True, height=300)

if __name__ == '__main__':
    main()
